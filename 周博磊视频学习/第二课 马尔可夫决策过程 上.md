# 第二课 马尔可夫决策过程 上

1. 马尔可夫过程

   * 状态的历史：$h_{t}=\{s_{1},s_{2},……,s_{t}\}$,$s_{t}$是马尔可夫的当且仅当
     $$
     p(s_{t+1}|s_{t})=p(s_{t+1}|h_{t})\qquad p(s_{t+1}|s_{t},a_{t})=p(s_{t+1}|h_{t},a_{t})
     $$

   * 状态转移矩阵：
     $$
     P=\begin{bmatrix}
     p(s_{1}|s_{1}) &\cdots   &p(s_{1}|s_{N}) \\ 
     \vdots  &  &\vdots  \\ 
     p(s_{N}|s_{1}) &\cdots   &p(s_{N}|s_{N}) 
     \end{bmatrix}
     $$

2. 马尔可夫奖励过程（$MRP$）

   * $MRP$组成：一列状态$S$；状态转移矩阵$P$；奖励方程（$reward\ function$）$R:$$R(S_{t}=s)=E(r_{t}|s_{t}=s)$；折扣因子（$discount\ factor$）$\gamma $

   * $horizon$：每个$episode$中$agent$走过的步数

   * $return$：从时间$t$到$horizon$的奖励$R$的折扣加和
     $$
     G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\cdots+\gamma^{T-t-1}R_{T} 
     $$

   * $MRP$​的价值函数
     $$
      V_{t}(s)=E\left[G_{t} \mid s_{t}=s\right]=E\left[R_{t+1}+\cdots+\gamma^{T-t-1}R_{T}| s_{t}=s\right] 
     $$
     价值函数满足$bellman$​等式
     $$
     V(s)=R(s)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)
     $$
     
     等式的矩阵形式
     $$
     \begin{bmatrix}
      V(s_{1}) \\ 
      \vdots\\ 
      V(s_{N})
     \end{bmatrix}=
     \begin{bmatrix}
      R(s_{1}) \\ 
      \vdots\\ 
      R(s_{N})
     \end{bmatrix}+\gamma
     \begin{bmatrix}
     p(s_{1}|s_{1}) &\cdots   &p(s_{N}|s_{1}) \\ 
     \vdots  &  &\vdots  \\ 
     p(s_{1}|s_{N}) &\cdots   &p(s_{N}|s_{N}) 
     \end{bmatrix}
     \begin{bmatrix}
      V(s_{1}) \\ 
      \vdots\\ 
      V(s_{N})
     \end{bmatrix}
     $$
     即$V=R+\gamma PV$，可求得$V=(I-\gamma P)^{-1}R$
     
   * 用蒙特卡洛计算$MRP$的价值函数
     $$
     \begin{align}
     &i\gets  0,G_{t}\gets 0\\
     &while\ i\ne N\ do\\
     &\qquad 生成一个episode,从状态s,时间t开始\\
     &\qquad 计算return\ g=\sum _{i=t} ^{H-1} \gamma^{i-t} r_{i}\\
     &\qquad G_{t}\gets G_{t}+g,i\gets i+1\\
     &end\  while\\
     &V_{t}(s)\gets \frac{G_{t}}{N}
     \end{align}
     $$
   
   * 用迭代法计算$MRP$的价值函数
     $$
     \begin{align}
     &for\  all\  state\  s\in S\\
     &while\  \left \| V-V^{'} \right \| >\epsilon \  do\\
     &\qquad V\gets V^{'}\\
     &\qquad for\  all\  state\  s\in S\\
     &\qquad \qquad V^{'}(s)=R(s)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)\\
     &end\  while\\
     &return\  V^{'}(s)\  for\  all\  s\in S
     \end{align}
     $$
   
3. 蒙特卡洛决策过程（$MDP$）

   * $MDP$组成：一列状态$S$；一列行为$A$（$action$）；状态转移矩阵$P^{a}$，矩阵中每个元素为$p(s_{t+1}|s_{t}=s,a_{t}=a)$；奖励方程（$reward\ function$）$R:$$R(S_{t}=s,a_{t}=a)=E(r_{t}|s_{t}=s,a_{t}=a)$；折扣因子（$discount\ factor$）$\gamma $

   * $MDP$的$policy$指明每个状态的$action$：$\pi (a|s)=p(a_{t}=a|s_{t}=s)$

   * $MDP$​转$MRP$​：给定$MDP(S,A,P,R,\gamma)$​和$policy\ \pi $​，序列$S_{1}S_{2}\cdots$​是一个马尔可夫过程，序列$S_1R_1S_2R_2\cdots$​是一个$MRP(S,P^\pi,R^\pi,\gamma)$​，其中
     $$
     p^\pi (s^{'}|s)=\sum_{a\in A}\pi (a|s)p(s^{'}|s,a)\\
     R^\pi (s)=\sum_{a\in A}\pi (a|s)R(s,a)
     $$

   * $MRP$由$s^{'}$决定$s$，$MDP$由$s$决定$a$再决定$s$

   * $MDP$的价值函数：

     * $state\ value\ function$：$v^\pi (s)=E_{\pi}(G_{t}|S_{t}=s)$
     * $action\ value\ function$：$q^\pi (s,a)=E_{\pi}(G_t|S_t=s,A_t=a)$
     * 两者关系：$v^\pi (s)=\sum_{a\in A}\pi (a|s)q^\pi (s,a)$
     * 两个价值函数期望的下标意思是对$\pi$取样

   * $Bellman$等式：
     $$
     v^{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) v^{\pi}\left(s^{\prime}\right)\right)\\
     q^{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) q^{\pi}(s^{\prime}|a^{\prime})
     $$
     
