# 第三课 无模型价值函数估计和控制 下

1. $generalized\  policy\ iteration\ (GPI)$

2. $MC\ with\ exploring\ start$

   * 为保证$PI$收敛，要求$episode$有$ES$，即每个行为在$episode$行走无限次后总能被取到

   * 具体算法
     $$
     \begin{align}
     &for\ all\ s\in S,initialize\ \pi (s)\in \mathcal{A}(s)\ arbitrarily\\
     &initialize\ Q(s,a)\in \mathbb{R}\ for\ all\ s\in S\ and\ a\in \mathcal{A}(s)\\
     &Returns(s,a)\gets empty\ list\ for\ all\ s\in S\ and\ a\in \mathcal{A}(s)\\
     &loop\ forever:\\
     &\qquad 任取S_0\in S,A_0\in \mathcal{A}(s)(其中所有序对取到的概率大于0)\\
     &\qquad 以\pi 为策略从,S_0,A_0开始生成一个episode:S_0A_0R_1\cdots S_{T-1}A_{T-1}R_{T}\\
     &\qquad for\ each\ step\ of\ episode\ t=T-1,T-2\cdots ,1,0:\\
     &\qquad \qquad G\gets \gamma G+R_{t+1}\\
     &\qquad \qquad 若S_t,A_t没出现在S_0,A_0,\cdots ,S_{t-1},A_{t-1}中：\\
     &\qquad \qquad \qquad append\ G\ to\ Returns(S_t,A_t)\\
     &\qquad \qquad \qquad Q(S_t,A_t)\gets average(Returns(S_t,A_t))\\
     &\qquad \qquad \qquad \pi (S_t)\gets \underset{a}{argmax}\ Q(S_t,a)
     \end{align}
     $$

3. $\epsilon -greedy\ exploration$

   * 确保不断的$explore$，即使用新的行为

   * 具体地：

     * 所有行动出现概率大于0

     * 有$1-\epsilon$的概率选择$greedy\ action$

     * 有$\epsilon$的几率随机选择行为（$explore$）
       $$
       \pi (a|s)=\left\{\begin{matrix}
       \frac{\epsilon }{\left | A \right | }+1-\epsilon    &if\ a^{\star }=\underset{a\in A}{argmax}\ Q(s,a)  \\
       \frac{\epsilon }{\left | A \right | }   &else
       \end{matrix}\right.
       $$

   * $policy\ improvement$保证$\epsilon -greedy\ exploration$单调递增
     $$
     \begin{align}
     q_{\pi}(s,\pi^{\prime}(s,a))&  = \sum_{a\in A} \pi ^{\prime}(a|s)q_{\pi}(s,a)\\
     &=\frac{\epsilon}{\left | A\right |}\sum_{a\in A}q_{\pi}(s,a)+(1-\epsilon)\underset{a}{max}\ q_{\pi}(s,a)\\
     &\ge \frac{\epsilon}{\left | A\right |}\sum_{a\in A}q_{\pi}(s,a)+(1-\epsilon)\sum_{a\in A}\frac{\pi(a|s)-\frac{\epsilon}{\left | A\right |}}{1-\epsilon} q_{\pi}(s,a)\\
     &=\sum_{a\in A}\pi (a|s) q_{\pi}(s,a)\\
     &=v_{pi}(s)\\
     \end{align}
     $$

     $$
     \Rightarrow v_{\pi^{\prime}}(s)=max\ q_{\pi}(s,\pi^{\prime}(s))\ge v_{\pi}(s)
     $$

4. $MC\ with\ \epsilon -greedy\ exploration$
   $$
   \begin{align}
   &initialize\ Q(S,A)=0,N(S,A)=0,\epsilon =1,k=1\\
   &\pi _k=\epsilon -greedy(Q)\\
   &loop\\
   &\qquad generalize\ k_{th}\ episode\ (S_1A_1R_2\cdots S_T)\sim \pi_k\\
   &\qquad for\ each\ S_t,A_t\ in episode\ do:\\
   &\qquad \qquad N(S_t,A_t)\gets N(S_t,A_t)+1\\
   &\qquad \qquad Q(S_t,A_t)\gets Q(S_t,A_t)+\frac{1}{N(S_t,A_t)}(G_t-Q(S_t,A_t))\\
   &\qquad end\ for\\
   &\qquad k\gets k+1,\epsilon \gets \frac{1}{k}\\
   &\qquad \pi_k=\epsilon -greedy(Q)\\
   &end\ loop
   \end{align}
   $$

5. 用$TD$代替$MC$，并使用$\epsilon -greedy$，得到$sarsa$算法（$on-policy\ TD\ control$）
   $$
   \begin{align}
   &initialize\ Q(s,a)\ for\ all\ s\in S\ and\ a\in \mathcal{A}(s)\ and\ Q(terminal\ state,\cdot)=0\\
   &for\ each\ episode:\\
   &\qquad initialize\ S\\
   &\qquad choose\ A\ through\ S\ and\ Q(s,a)\ and\ \epsilon -greedy\\
   &\qquad for\ each\ step\ of\ episode:\\
   &\qquad \qquad take\ action\ A\ and\ get\ R,S^{\prime}\\
   &\qquad \qquad choose\ A^{\prime} \ through\ S^{\prime}\ and\ Q\\
   &\qquad \qquad Q(S,A)\gets Q(S,A)+\alpha (R+\gamma Q(S^{\prime},A^{\prime})-Q(S,A))\\
   &\qquad \qquad S\gets S^{\prime},A\gets A^{\prime}\\
   &\qquad until\ terminal\ state\\
   \end{align}
   $$

6. $n-step\ sarsa$：调整$sarsa$向前步数（$n=1$即为普通$sarsa$）
   $$
   \begin{align}
   &n=1:q_{t}^{(1)}=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})\\
   &n=2:q_{t}^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma ^{2}Q(S_{t+2},A_{t+2})\\
   &\vdots\\
   &n=\infty:q_{t}^{(\infty)}=R_{t+1}+\gamma R_{t+2}+\cdots +\gamma ^{T-t-1}R_{T}\ (MC)\\
   \end{align}
   $$
   其对于$Q$函数的改进对应地变成$Q(S_t,A_t)\gets Q(S_t,A_t)+\alpha (q_{t}^{(n)}-Q(S_t,A_t))$

7. $on-policy\ learning\ vs\ off-policy\ learning$

   * $on-policy$：通过从$\pi$中收到的经验学习$\pi$，为了$explore$所有行为，可能并非最佳，然后不断减少$explore$的程度
   * $off-policy$：通过从策略$\mu$中收到的经验学习策略$\pi$
     * $\mu $：行为策略，更加$explore$，且为生成轨迹的策略；$\pi$：目标策略，待学习并成为最优策略
     * 具体地：$S_1A_1R_2\cdots S_T\sim \mu$，用$S_1A_1R_2\cdots S_T$更新$\pi$
     * 优势
       * 跟随更加探索的策略学习得到最佳策略
       * 可以从人的观察或者其他智能体上进行学习
       * 可重复利用其它旧策略$\pi_1,\pi_2,\cdots$
   
8. $off-policy\ learning\ with\ Q-learning$

   * $target\ policy\ \pi$：$greedy\ on\ Q(S,A)$，即$\pi (S_{t+1})=\underset{A^{\prime}}{argmax\ } Q(S_{t+1},A^{\prime})$

   * $behavior\ policy\ \mu$：跟随$\epsilon -greedy$的$Q(S,A)$进行提升

   * 具体地：
     $$
     \begin{align}
     R_{t+1}+\gamma Q(S_{t+1},A^{\prime})&=R_{t+1}+\gamma Q(S_{t+1},\underset{A^{\prime}}{argmax\ }Q(S_{t+1},A^{\prime}))\\
     &=R_{t+1}+\gamma \underset{A^{\prime}}{max\ }Q(S_{t+1},A^{\prime})
     \end{align}
     $$

   * 具体算法和上面的$on-policy$大致相同，区别在于如何更新$Q$函数，需要将$TD\ target$改成本标号下上面所述的$TD\ target$